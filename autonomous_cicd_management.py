"""
Autonomous CI/CD Management System
===================================
Centralized command and control for managing 1-10,000+ deployments
with continuous testing, predictive maintenance, and autonomous orchestration.

Enables:
- Single-point control for all deployments (Render, Vercel, Docker Hub, etc.)
- Continuous exhaustive testing with rapid iteration
- Predictive deployment scheduling based on traffic/usage patterns
- Self-healing rollback on failures
- Multi-service coordination for complex deployments

Based on 2025 best practices from Google SRE, Netflix deployment automation, and GitOps principles.
"""

import asyncio
import hashlib
import json
import logging
import os
from dataclasses import asdict, dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Optional

import aiohttp

logger = logging.getLogger(__name__)



# Connection pool helper - prefer shared pool, fallback to direct connection
async def _get_db_connection(db_url: str = None):
    """Get database connection, preferring shared pool"""
    try:
        from database.async_connection import get_pool
        pool = get_pool()
        return await pool.acquire()
    except Exception as exc:
        logger.warning("Shared pool unavailable, falling back to direct connection: %s", exc, exc_info=True)
        # Fallback to direct connection if pool unavailable
        if db_url:
            import asyncpg
            return await asyncpg.connect(db_url)
        return None

class DeploymentPlatform(Enum):
    """Supported deployment platforms"""
    RENDER = "render"
    VERCEL = "vercel"
    DOCKER_HUB = "docker_hub"
    AWS_ECS = "aws_ecs"
    GCP_CLOUD_RUN = "gcp_cloud_run"
    KUBERNETES = "kubernetes"
    RAILWAY = "railway"
    FLY_IO = "fly_io"
    HEROKU = "heroku"
    CUSTOM = "custom"


class DeploymentStatus(Enum):
    """Deployment lifecycle status"""
    PENDING = "pending"
    BUILDING = "building"
    TESTING = "testing"
    DEPLOYING = "deploying"
    LIVE = "live"
    ROLLING_BACK = "rolling_back"
    FAILED = "failed"
    CANCELLED = "cancelled"


class TestType(Enum):
    """Types of automated tests"""
    UNIT = "unit"
    INTEGRATION = "integration"
    E2E = "e2e"
    LOAD = "load"
    SECURITY = "security"
    CANARY = "canary"
    SMOKE = "smoke"


@dataclass
class Service:
    """A deployable service"""
    service_id: str
    name: str
    platform: DeploymentPlatform
    repository: str
    branch: str = "main"
    environment: str = "production"
    current_version: str = ""
    health_endpoint: str = "/health"
    deployment_url: Optional[str] = None
    platform_config: dict[str, Any] = field(default_factory=dict)
    dependencies: list[str] = field(default_factory=list)  # service_ids this depends on
    last_deployed: Optional[str] = None
    last_health_check: Optional[str] = None
    health_status: str = "unknown"


@dataclass
class Deployment:
    """A deployment operation"""
    deployment_id: str
    service_id: str
    version: str
    status: DeploymentStatus
    started_at: str
    completed_at: Optional[str] = None
    triggered_by: str = "autonomous"  # autonomous, manual, webhook
    commit_sha: Optional[str] = None
    build_logs: str = ""
    test_results: dict[str, Any] = field(default_factory=dict)
    rollback_version: Optional[str] = None
    duration_seconds: Optional[float] = None
    error: Optional[str] = None


@dataclass
class TestRun:
    """An automated test run"""
    test_id: str
    deployment_id: str
    test_type: TestType
    status: str  # pending, running, passed, failed
    started_at: str
    completed_at: Optional[str] = None
    results: dict[str, Any] = field(default_factory=dict)
    coverage: Optional[float] = None


@dataclass
class DeploymentSchedule:
    """Scheduled deployment based on traffic patterns"""
    schedule_id: str
    service_id: str
    cron_expression: str
    deployment_window: dict[str, str]  # {"start": "02:00", "end": "04:00"}
    conditions: dict[str, Any]  # traffic thresholds, etc.
    enabled: bool = True
    last_executed: Optional[str] = None


class AutonomousCICDManagement:
    """
    Autonomous CI/CD Management Engine

    Capabilities:
    - Manage 1 to 10,000+ services from single control plane
    - Continuous testing on every change
    - Predictive deployment scheduling
    - Automatic rollback on failures
    - Multi-service coordinated deployments
    - Integration with MCP Bridge for tool execution
    """

    def __init__(self):
        self.db_url = os.getenv("DATABASE_URL")
        self.services: dict[str, Service] = {}
        self.deployments: dict[str, Deployment] = {}
        self.schedules: dict[str, DeploymentSchedule] = {}
        self._initialized = False

        # Platform API configurations
        self.platform_configs = {
            DeploymentPlatform.RENDER: {
                "api_url": "https://api.render.com/v1",
                "api_key": os.getenv("RENDER_API_KEY"),
            },
            DeploymentPlatform.VERCEL: {
                "api_url": "https://api.vercel.com",
                "api_key": os.getenv("VERCEL_TOKEN"),
            },
            DeploymentPlatform.DOCKER_HUB: {
                "api_url": "https://hub.docker.com/v2",
                "username": os.getenv("DOCKER_USERNAME"),
                "password": os.getenv("DOCKER_PASSWORD"),
            },
        }

        # MCP Bridge integration - NO hardcoded credentials
        self.mcp_bridge_url = os.getenv("MCP_BRIDGE_URL", "https://brainops-mcp-bridge.onrender.com")
        self.mcp_api_key = os.getenv("BRAINOPS_API_KEY") or os.getenv("AGENTS_API_KEY")  # Required - no default

        # Metrics
        self.total_deployments = 0
        self.successful_deployments = 0
        self.auto_rollbacks = 0
        self.avg_deployment_time_seconds = 0

    async def initialize(self):
        """Initialize the CI/CD management system"""
        if self._initialized:
            return

        logger.info("Initializing Autonomous CI/CD Management System...")

        await self._create_tables()
        await self._load_services()
        await self._load_schedules()

        self._initialized = True
        logger.info(f"CI/CD Management initialized with {len(self.services)} services")

    async def _create_tables(self):
        """Create database tables"""
        try:
            import asyncpg
            if not self.db_url:
                return

            conn = await asyncpg.connect(self.db_url)
            try:
                await conn.execute("""
                    CREATE TABLE IF NOT EXISTS cicd_services (
                        service_id TEXT PRIMARY KEY,
                        name TEXT NOT NULL,
                        platform TEXT NOT NULL,
                        repository TEXT NOT NULL,
                        branch TEXT DEFAULT 'main',
                        environment TEXT DEFAULT 'production',
                        current_version TEXT,
                        health_endpoint TEXT DEFAULT '/health',
                        deployment_url TEXT,
                        platform_config JSONB DEFAULT '{}',
                        dependencies JSONB DEFAULT '[]',
                        last_deployed TIMESTAMPTZ,
                        last_health_check TIMESTAMPTZ,
                        health_status TEXT DEFAULT 'unknown',
                        created_at TIMESTAMPTZ DEFAULT NOW(),
                        updated_at TIMESTAMPTZ DEFAULT NOW()
                    );

                    CREATE TABLE IF NOT EXISTS cicd_deployments (
                        deployment_id TEXT PRIMARY KEY,
                        service_id TEXT NOT NULL,
                        version TEXT NOT NULL,
                        status TEXT NOT NULL,
                        started_at TIMESTAMPTZ DEFAULT NOW(),
                        completed_at TIMESTAMPTZ,
                        triggered_by TEXT DEFAULT 'autonomous',
                        commit_sha TEXT,
                        build_logs TEXT,
                        test_results JSONB DEFAULT '{}',
                        rollback_version TEXT,
                        duration_seconds FLOAT,
                        error TEXT,
                        created_at TIMESTAMPTZ DEFAULT NOW()
                    );

                    CREATE TABLE IF NOT EXISTS cicd_test_runs (
                        test_id TEXT PRIMARY KEY,
                        deployment_id TEXT NOT NULL,
                        test_type TEXT NOT NULL,
                        status TEXT NOT NULL,
                        started_at TIMESTAMPTZ DEFAULT NOW(),
                        completed_at TIMESTAMPTZ,
                        results JSONB DEFAULT '{}',
                        coverage FLOAT
                    );

                    CREATE TABLE IF NOT EXISTS cicd_schedules (
                        schedule_id TEXT PRIMARY KEY,
                        service_id TEXT NOT NULL,
                        cron_expression TEXT NOT NULL,
                        deployment_window JSONB DEFAULT '{}',
                        conditions JSONB DEFAULT '{}',
                        enabled BOOLEAN DEFAULT TRUE,
                        last_executed TIMESTAMPTZ
                    );

                    CREATE INDEX IF NOT EXISTS idx_cicd_deployments_service
                        ON cicd_deployments(service_id, started_at DESC);
                    CREATE INDEX IF NOT EXISTS idx_cicd_deployments_status
                        ON cicd_deployments(status);
                """)
            finally:
                await conn.close()

            logger.info("CI/CD tables created/verified")
        except Exception as e:
            logger.error(f"Failed to create CI/CD tables: {e}")

    async def _load_services(self):
        """Load services from database"""
        try:
            import asyncpg
            if not self.db_url:
                return

            conn = await asyncpg.connect(self.db_url)
            try:
                rows = await conn.fetch("SELECT * FROM cicd_services")
                for row in rows:
                    service = Service(
                        service_id=row["service_id"],
                        name=row["name"],
                        platform=DeploymentPlatform(row["platform"]),
                        repository=row["repository"],
                        branch=row["branch"] or "main",
                        environment=row["environment"] or "production",
                        current_version=row["current_version"] or "",
                        health_endpoint=row["health_endpoint"] or "/health",
                        deployment_url=row["deployment_url"],
                        platform_config=json.loads(row["platform_config"]) if row["platform_config"] else {},
                        dependencies=json.loads(row["dependencies"]) if row["dependencies"] else [],
                        last_deployed=row["last_deployed"].isoformat() if row["last_deployed"] else None,
                        health_status=row["health_status"] or "unknown",
                    )
                    self.services[service.service_id] = service
            finally:
                await conn.close()
        except Exception as e:
            logger.error(f"Failed to load services: {e}")

    async def _load_schedules(self):
        """Load deployment schedules from database"""
        try:
            import asyncpg
            if not self.db_url:
                return

            conn = await asyncpg.connect(self.db_url)
            try:
                rows = await conn.fetch("SELECT * FROM cicd_schedules WHERE enabled = TRUE")
                for row in rows:
                    schedule = DeploymentSchedule(
                        schedule_id=row["schedule_id"],
                        service_id=row["service_id"],
                        cron_expression=row["cron_expression"],
                        deployment_window=json.loads(row["deployment_window"]) if row["deployment_window"] else {},
                        conditions=json.loads(row["conditions"]) if row["conditions"] else {},
                        enabled=row["enabled"],
                        last_executed=row["last_executed"].isoformat() if row["last_executed"] else None,
                    )
                    self.schedules[schedule.schedule_id] = schedule
            finally:
                await conn.close()
        except Exception as e:
            logger.error(f"Failed to load schedules: {e}")

    async def register_service(
        self,
        name: str,
        platform: DeploymentPlatform,
        repository: str,
        **kwargs
    ) -> Service:
        """Register a new service for CI/CD management"""
        service_id = hashlib.md5(f"{name}:{platform.value}:{repository}".encode()).hexdigest()[:12]

        service = Service(
            service_id=service_id,
            name=name,
            platform=platform,
            repository=repository,
            **kwargs
        )

        self.services[service_id] = service

        # Persist to database
        await self._save_service(service)

        logger.info(f"Registered service: {name} on {platform.value}")
        return service

    async def _save_service(self, service: Service):
        """Save service to database"""
        try:
            import asyncpg
            if not self.db_url:
                return

            conn = await asyncpg.connect(self.db_url)
            try:
                await conn.execute("""
                    INSERT INTO cicd_services (
                        service_id, name, platform, repository, branch, environment,
                        current_version, health_endpoint, deployment_url, platform_config,
                        dependencies, health_status
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)
                    ON CONFLICT (service_id) DO UPDATE SET
                        name = EXCLUDED.name,
                        platform = EXCLUDED.platform,
                        repository = EXCLUDED.repository,
                        current_version = EXCLUDED.current_version,
                        health_status = EXCLUDED.health_status,
                        updated_at = NOW()
                """, service.service_id, service.name, service.platform.value,
                    service.repository, service.branch, service.environment,
                    service.current_version, service.health_endpoint, service.deployment_url,
                    json.dumps(service.platform_config), json.dumps(service.dependencies),
                    service.health_status)
            finally:
                await conn.close()
        except Exception as e:
            logger.error(f"Failed to save service: {e}")

    async def deploy_service(
        self,
        service_id: str,
        version: Optional[str] = None,
        triggered_by: str = "autonomous"
    ) -> Optional[Deployment]:
        """Deploy a service to its platform"""
        service = self.services.get(service_id)
        if not service:
            logger.error(f"Service not found: {service_id}")
            return None

        deployment_id = hashlib.md5(
            f"{service_id}:{datetime.utcnow().isoformat()}".encode()
        ).hexdigest()[:16]

        deployment = Deployment(
            deployment_id=deployment_id,
            service_id=service_id,
            version=version or datetime.utcnow().strftime("%Y%m%d.%H%M%S"),
            status=DeploymentStatus.PENDING,
            started_at=datetime.utcnow().isoformat(),
            triggered_by=triggered_by,
        )

        self.deployments[deployment_id] = deployment
        self.total_deployments += 1

        try:
            # Run pre-deployment tests
            deployment.status = DeploymentStatus.TESTING
            test_passed = await self._run_tests(deployment)

            if not test_passed:
                deployment.status = DeploymentStatus.FAILED
                deployment.error = "Pre-deployment tests failed"
                deployment.completed_at = datetime.utcnow().isoformat()
                await self._save_deployment(deployment)
                return deployment

            # Execute deployment via MCP Bridge or direct API
            deployment.status = DeploymentStatus.DEPLOYING
            success = await self._execute_deployment(service, deployment)

            if success:
                deployment.status = DeploymentStatus.LIVE
                deployment.completed_at = datetime.utcnow().isoformat()
                deployment.duration_seconds = (
                    datetime.fromisoformat(deployment.completed_at) -
                    datetime.fromisoformat(deployment.started_at)
                ).total_seconds()

                service.current_version = deployment.version
                service.last_deployed = deployment.completed_at
                await self._save_service(service)

                self.successful_deployments += 1
                logger.info(f"âœ… Deployed {service.name} v{deployment.version}")
            else:
                # Auto-rollback on failure
                await self._auto_rollback(service, deployment)

            await self._save_deployment(deployment)
            return deployment

        except Exception as e:
            logger.error(f"Deployment failed: {e}")
            deployment.status = DeploymentStatus.FAILED
            deployment.error = str(e)
            deployment.completed_at = datetime.utcnow().isoformat()
            await self._save_deployment(deployment)
            return deployment

    async def _run_tests(self, deployment: Deployment) -> bool:
        """Run pre-deployment tests"""
        test_types = [TestType.UNIT, TestType.INTEGRATION, TestType.SMOKE]
        all_passed = True

        for test_type in test_types:
            test_id = f"{deployment.deployment_id}_{test_type.value}"
            test_run = TestRun(
                test_id=test_id,
                deployment_id=deployment.deployment_id,
                test_type=test_type,
                status="running",
                started_at=datetime.utcnow().isoformat(),
            )

            try:
                # Execute test via MCP Bridge
                result = await self._execute_via_mcp(
                    "run_tests",
                    {
                        "test_type": test_type.value,
                        "service_id": deployment.service_id,
                        "version": deployment.version,
                    }
                )

                test_run.status = "passed" if result.get("passed", False) else "failed"
                test_run.results = result
                test_run.completed_at = datetime.utcnow().isoformat()

                if test_run.status == "failed":
                    all_passed = False
                    logger.warning(f"âŒ {test_type.value} tests failed")

            except Exception as e:
                test_run.status = "failed"
                test_run.results = {"error": str(e)}
                all_passed = False

            deployment.test_results[test_type.value] = asdict(test_run)

        return all_passed

    async def _execute_deployment(self, service: Service, deployment: Deployment) -> bool:
        """Execute the actual deployment"""
        try:
            if service.platform == DeploymentPlatform.RENDER:
                return await self._deploy_to_render(service, deployment)
            elif service.platform == DeploymentPlatform.VERCEL:
                return await self._deploy_to_vercel(service, deployment)
            elif service.platform == DeploymentPlatform.DOCKER_HUB:
                return await self._deploy_docker(service, deployment)
            else:
                # Use MCP Bridge for other platforms
                return await self._deploy_via_mcp(service, deployment)
        except Exception as e:
            logger.error(f"Deployment execution failed: {e}")
            deployment.error = str(e)
            return False

    async def _deploy_to_render(self, service: Service, deployment: Deployment) -> bool:
        """Deploy to Render using API or MCP Bridge"""
        config = self.platform_configs.get(DeploymentPlatform.RENDER, {})
        api_key = config.get("api_key")

        if not api_key:
            # Fallback to MCP Bridge
            return await self._deploy_via_mcp(service, deployment)

        try:
            async with aiohttp.ClientSession() as session:
                # Trigger deployment
                headers = {"Authorization": f"Bearer {api_key}"}
                deploy_url = f"{config['api_url']}/services/{service.platform_config.get('service_id')}/deploys"

                async with session.post(deploy_url, headers=headers) as resp:
                    if resp.status in (200, 201):
                        result = await resp.json()
                        deployment.build_logs = f"Deploy triggered: {result.get('id')}"
                        return True
                    else:
                        error = await resp.text()
                        deployment.error = f"Render API error: {error}"
                        return False
        except Exception as e:
            logger.error(f"Render deployment failed: {e}")
            deployment.error = str(e)
            return False

    async def _deploy_to_vercel(self, service: Service, deployment: Deployment) -> bool:
        """Deploy to Vercel using API or MCP Bridge"""
        config = self.platform_configs.get(DeploymentPlatform.VERCEL, {})
        api_key = config.get("api_key")

        if not api_key:
            return await self._deploy_via_mcp(service, deployment)

        try:
            async with aiohttp.ClientSession() as session:
                headers = {"Authorization": f"Bearer {api_key}"}
                # Create deployment
                deploy_data = {
                    "name": service.name,
                    "gitSource": {
                        "type": "github",
                        "repo": service.repository,
                        "ref": service.branch,
                    }
                }
                async with session.post(
                    f"{config['api_url']}/v13/deployments",
                    headers=headers,
                    json=deploy_data
                ) as resp:
                    if resp.status in (200, 201):
                        result = await resp.json()
                        deployment.build_logs = f"Vercel deploy: {result.get('id')}"
                        return True
                    else:
                        error = await resp.text()
                        deployment.error = f"Vercel API error: {error}"
                        return False
        except Exception as e:
            logger.error(f"Vercel deployment failed: {e}")
            deployment.error = str(e)
            return False

    async def _deploy_docker(self, service: Service, deployment: Deployment) -> bool:
        """Build and push Docker image"""
        try:
            result = await self._execute_via_mcp(
                "docker_build_push",
                {
                    "image": service.platform_config.get("image", service.name),
                    "tag": deployment.version,
                    "dockerfile": service.platform_config.get("dockerfile", "Dockerfile"),
                    "context": service.repository,
                }
            )
            return result.get("success", False)
        except Exception as e:
            logger.error(f"Docker deployment failed: {e}")
            deployment.error = str(e)
            return False

    async def _deploy_via_mcp(self, service: Service, deployment: Deployment) -> bool:
        """Deploy via MCP Bridge"""
        tool_map = {
            DeploymentPlatform.RENDER: "render_trigger_deploy",
            DeploymentPlatform.VERCEL: "vercel_create_deployment",
            DeploymentPlatform.DOCKER_HUB: "docker_push",
            DeploymentPlatform.KUBERNETES: "kubernetes_apply",
        }

        tool_name = tool_map.get(service.platform, "generic_deploy")
        result = await self._execute_via_mcp(
            tool_name,
            {
                "service": service.name,
                "version": deployment.version,
                "platform_config": service.platform_config,
            }
        )
        return result.get("success", False)

    async def _execute_via_mcp(self, tool_name: str, params: dict[str, Any]) -> dict[str, Any]:
        """Execute a tool via MCP Bridge"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.mcp_bridge_url}/mcp/execute",
                    headers={
                        "X-API-Key": self.mcp_api_key,
                        "Content-Type": "application/json",
                    },
                    json={
                        "server": self._get_mcp_server(tool_name),
                        "tool": tool_name,
                        "params": params,
                    },
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as resp:
                    if resp.status == 200:
                        return await resp.json()
                    else:
                        error = await resp.text()
                        logger.warning(f"MCP execution failed: {error}")
                        return {"success": False, "error": error}
        except Exception as e:
            logger.error(f"MCP Bridge call failed: {e}")
            return {"success": False, "error": str(e)}

    def _get_mcp_server(self, tool_name: str) -> str:
        """Determine which MCP server to use for a tool"""
        if tool_name.startswith("render"):
            return "render-mcp"
        elif tool_name.startswith("vercel"):
            return "vercel-mcp"
        elif tool_name.startswith("docker"):
            return "docker-mcp"
        elif tool_name.startswith("kubernetes"):
            return "kubernetes-mcp"
        else:
            return "generic-mcp"

    async def _auto_rollback(self, service: Service, deployment: Deployment):
        """Automatically rollback on deployment failure"""
        if not service.current_version:
            logger.warning(f"No previous version to rollback for {service.name}")
            return

        deployment.status = DeploymentStatus.ROLLING_BACK
        deployment.rollback_version = service.current_version

        logger.warning(f"ðŸ”„ Auto-rolling back {service.name} to {service.current_version}")

        # Create rollback deployment
        rollback = await self.deploy_service(
            service.service_id,
            version=service.current_version,
            triggered_by="auto_rollback"
        )

        if rollback and rollback.status == DeploymentStatus.LIVE:
            deployment.status = DeploymentStatus.FAILED
            self.auto_rollbacks += 1
            logger.info(f"âœ… Rollback successful for {service.name}")
        else:
            deployment.status = DeploymentStatus.FAILED
            deployment.error = "Rollback also failed!"
            logger.error(f"âŒ Rollback failed for {service.name}")

    async def _save_deployment(self, deployment: Deployment):
        """Save deployment to database"""
        try:
            import asyncpg
            if not self.db_url:
                return

            conn = await asyncpg.connect(self.db_url)
            try:
                await conn.execute("""
                    INSERT INTO cicd_deployments (
                        deployment_id, service_id, version, status, started_at,
                        completed_at, triggered_by, commit_sha, build_logs,
                        test_results, rollback_version, duration_seconds, error
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13)
                    ON CONFLICT (deployment_id) DO UPDATE SET
                        status = EXCLUDED.status,
                        completed_at = EXCLUDED.completed_at,
                        build_logs = EXCLUDED.build_logs,
                        test_results = EXCLUDED.test_results,
                        rollback_version = EXCLUDED.rollback_version,
                        duration_seconds = EXCLUDED.duration_seconds,
                        error = EXCLUDED.error
                """, deployment.deployment_id, deployment.service_id, deployment.version,
                    deployment.status.value, deployment.started_at,
                    deployment.completed_at, deployment.triggered_by, deployment.commit_sha,
                    deployment.build_logs, json.dumps(deployment.test_results),
                    deployment.rollback_version, deployment.duration_seconds, deployment.error)
            finally:
                await conn.close()
        except Exception as e:
            logger.error(f"Failed to save deployment: {e}")

    async def check_all_health(self) -> dict[str, Any]:
        """Check health of all registered services"""
        results = {}

        async with aiohttp.ClientSession() as session:
            tasks = []
            for service_id, service in self.services.items():
                tasks.append(self._check_service_health(session, service))

            health_results = await asyncio.gather(*tasks, return_exceptions=True)

            for service_id, result in zip(self.services.keys(), health_results):
                if isinstance(result, Exception):
                    results[service_id] = {"status": "error", "error": str(result)}
                else:
                    results[service_id] = result

        return results

    async def _check_service_health(
        self,
        session: aiohttp.ClientSession,
        service: Service
    ) -> dict[str, Any]:
        """Check health of a single service"""
        if not service.deployment_url:
            return {"status": "unknown", "reason": "No deployment URL"}

        try:
            url = f"{service.deployment_url.rstrip('/')}{service.health_endpoint}"
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                status = "healthy" if resp.status == 200 else "unhealthy"
                service.health_status = status
                service.last_health_check = datetime.utcnow().isoformat()
                return {
                    "status": status,
                    "http_status": resp.status,
                    "checked_at": service.last_health_check,
                }
        except Exception as e:
            service.health_status = "error"
            return {"status": "error", "error": str(e)}

    async def detect_performance_regression(
        self,
        service_id: str,
        current_metrics: dict[str, Any],
        baseline_metrics: dict[str, Any]
    ) -> dict[str, Any]:
        """Detect performance regressions by comparing current vs baseline metrics"""
        try:
            regressions = []
            improvements = []

            # Compare response times
            if "avg_response_time_ms" in current_metrics and "avg_response_time_ms" in baseline_metrics:
                current_rt = current_metrics["avg_response_time_ms"]
                baseline_rt = baseline_metrics["avg_response_time_ms"]
                diff_percent = ((current_rt - baseline_rt) / baseline_rt * 100) if baseline_rt > 0 else 0

                if diff_percent > 20:  # 20% slower
                    regressions.append({
                        "metric": "response_time",
                        "current": f"{current_rt}ms",
                        "baseline": f"{baseline_rt}ms",
                        "degradation": f"{diff_percent:.1f}%",
                        "severity": "high" if diff_percent > 50 else "medium"
                    })
                elif diff_percent < -10:  # 10% faster
                    improvements.append({
                        "metric": "response_time",
                        "improvement": f"{abs(diff_percent):.1f}%"
                    })

            # Compare error rates
            if "error_rate" in current_metrics and "error_rate" in baseline_metrics:
                current_er = current_metrics["error_rate"]
                baseline_er = baseline_metrics["error_rate"]
                diff = current_er - baseline_er

                if diff > 0.5:  # 0.5% increase in errors
                    regressions.append({
                        "metric": "error_rate",
                        "current": f"{current_er}%",
                        "baseline": f"{baseline_er}%",
                        "increase": f"{diff:.2f}%",
                        "severity": "critical" if diff > 2 else "high"
                    })

            # Compare throughput
            if "requests_per_second" in current_metrics and "requests_per_second" in baseline_metrics:
                current_rps = current_metrics["requests_per_second"]
                baseline_rps = baseline_metrics["requests_per_second"]
                diff_percent = ((current_rps - baseline_rps) / baseline_rps * 100) if baseline_rps > 0 else 0

                if diff_percent < -15:  # 15% throughput drop
                    regressions.append({
                        "metric": "throughput",
                        "current": f"{current_rps} req/s",
                        "baseline": f"{baseline_rps} req/s",
                        "degradation": f"{abs(diff_percent):.1f}%",
                        "severity": "high"
                    })

            # Compare memory usage
            if "memory_mb" in current_metrics and "memory_mb" in baseline_metrics:
                current_mem = current_metrics["memory_mb"]
                baseline_mem = baseline_metrics["memory_mb"]
                diff_percent = ((current_mem - baseline_mem) / baseline_mem * 100) if baseline_mem > 0 else 0

                if diff_percent > 30:  # 30% memory increase
                    regressions.append({
                        "metric": "memory_usage",
                        "current": f"{current_mem}MB",
                        "baseline": f"{baseline_mem}MB",
                        "increase": f"{diff_percent:.1f}%",
                        "severity": "medium"
                    })

            # Determine overall status
            has_critical = any(r.get("severity") == "critical" for r in regressions)
            has_high = any(r.get("severity") == "high" for r in regressions)

            if has_critical:
                status = "critical_regression"
                action = "Immediate rollback recommended"
            elif has_high and len(regressions) >= 2:
                status = "significant_regression"
                action = "Consider rollback or hotfix"
            elif len(regressions) > 0:
                status = "minor_regression"
                action = "Monitor closely"
            else:
                status = "no_regression"
                action = "Deployment performing as expected"

            return {
                "service_id": service_id,
                "status": status,
                "action": action,
                "regressions": regressions,
                "improvements": improvements,
                "regression_count": len(regressions),
                "evaluated_at": datetime.utcnow().isoformat()
            }

        except Exception as e:
            logger.error(f"Performance regression detection failed: {e}")
            return {
                "error": str(e),
                "status": "unknown"
            }

    async def advanced_deployment_monitoring(
        self,
        service_id: str,
        deployment_id: str,
        duration_minutes: int = 15
    ) -> dict[str, Any]:
        """Advanced monitoring after deployment with multiple checks"""
        try:
            service = self.services.get(service_id)
            if not service:
                return {"error": f"Service {service_id} not found"}

            monitoring_results = {
                "service_id": service_id,
                "deployment_id": deployment_id,
                "started_at": datetime.utcnow().isoformat(),
                "duration_minutes": duration_minutes,
                "checks": []
            }

            # Health check
            health_result = await self._check_service_health(
                aiohttp.ClientSession(),
                service
            )
            monitoring_results["checks"].append({
                "type": "health_check",
                "status": health_result.get("status"),
                "timestamp": datetime.utcnow().isoformat()
            })

            # Performance baseline comparison (simulated)
            current_metrics = {
                "avg_response_time_ms": 250,
                "error_rate": 0.3,
                "requests_per_second": 150,
                "memory_mb": 512
            }
            baseline_metrics = {
                "avg_response_time_ms": 200,
                "error_rate": 0.2,
                "requests_per_second": 180,
                "memory_mb": 450
            }

            regression_result = await self.detect_performance_regression(
                service_id,
                current_metrics,
                baseline_metrics
            )
            monitoring_results["regression_analysis"] = regression_result

            # Log analysis (simulated)
            log_analysis = {
                "error_count": 5,
                "warning_count": 12,
                "critical_issues": [],
                "patterns_detected": ["Increased database query time"]
            }
            monitoring_results["log_analysis"] = log_analysis

            # Overall assessment
            if regression_result.get("status") == "critical_regression":
                monitoring_results["overall_status"] = "failed"
                monitoring_results["recommendation"] = "Rollback immediately"
            elif regression_result.get("status") == "significant_regression":
                monitoring_results["overall_status"] = "degraded"
                monitoring_results["recommendation"] = "Investigate and consider rollback"
            elif health_result.get("status") == "unhealthy":
                monitoring_results["overall_status"] = "unhealthy"
                monitoring_results["recommendation"] = "Service is not responding correctly"
            else:
                monitoring_results["overall_status"] = "healthy"
                monitoring_results["recommendation"] = "Deployment successful, continue monitoring"

            return monitoring_results

        except Exception as e:
            logger.error(f"Advanced monitoring failed: {e}")
            return {
                "error": str(e),
                "overall_status": "error"
            }

    async def get_deployment_metrics(self) -> dict[str, Any]:
        """Get CI/CD metrics with enhanced analytics"""
        try:
            # Calculate deployment velocity
            recent_deployments = [d for d in self.deployments.values()
                                if d.started_at and
                                datetime.fromisoformat(d.started_at) > datetime.utcnow() - timedelta(days=7)]

            deployment_velocity = len(recent_deployments) / 7  # deployments per day

            # Calculate mean time to recovery (MTTR)
            rollback_deployments = [d for d in self.deployments.values()
                                  if d.triggered_by == "auto_rollback" and d.duration_seconds]
            avg_mttr = sum(d.duration_seconds for d in rollback_deployments) / len(rollback_deployments) if rollback_deployments else 0

            # Calculate change failure rate
            failed_deployments = [d for d in self.deployments.values()
                                if d.status == DeploymentStatus.FAILED]
            change_failure_rate = len(failed_deployments) / max(self.total_deployments, 1) * 100

            return {
                "total_services": len(self.services),
                "total_deployments": self.total_deployments,
                "successful_deployments": self.successful_deployments,
                "success_rate": (
                    self.successful_deployments / self.total_deployments * 100
                    if self.total_deployments > 0 else 100
                ),
                "auto_rollbacks": self.auto_rollbacks,
                "avg_deployment_time_seconds": self.avg_deployment_time_seconds,
                "deployment_velocity_per_day": round(deployment_velocity, 2),
                "mean_time_to_recovery_seconds": round(avg_mttr, 2),
                "change_failure_rate": round(change_failure_rate, 2),
                "services_by_platform": {
                    platform.value: len([s for s in self.services.values() if s.platform == platform])
                    for platform in DeploymentPlatform
                    if any(s.platform == platform for s in self.services.values())
                },
                "health_status": {
                    service.name: service.health_status
                    for service in self.services.values()
                },
                "dora_metrics": {
                    "deployment_frequency": f"{deployment_velocity:.1f} per day",
                    "lead_time_for_changes": f"{self.avg_deployment_time_seconds:.0f}s",
                    "mean_time_to_recovery": f"{avg_mttr:.0f}s",
                    "change_failure_rate": f"{change_failure_rate:.1f}%"
                }
            }
        except Exception as e:
            logger.error(f"Failed to get metrics: {e}")
            return {
                "error": str(e),
                "total_services": len(self.services),
                "total_deployments": self.total_deployments
            }

    async def deploy_all(self, triggered_by: str = "autonomous") -> list[Deployment]:
        """Deploy all services (coordinated multi-service deployment)"""
        deployments = []

        # Sort by dependencies (deploy dependencies first)
        ordered_services = self._topological_sort()

        for service_id in ordered_services:
            deployment = await self.deploy_service(service_id, triggered_by=triggered_by)
            if deployment:
                deployments.append(deployment)
                if deployment.status == DeploymentStatus.FAILED:
                    logger.error(f"Stopping deploy_all due to failure in {service_id}")
                    break

        return deployments

    def _topological_sort(self) -> list[str]:
        """Sort services by dependencies (deployment order)"""
        # Simple implementation - no dependencies first, then dependent services
        no_deps = [s.service_id for s in self.services.values() if not s.dependencies]
        with_deps = [s.service_id for s in self.services.values() if s.dependencies]
        return no_deps + with_deps


# Singleton instance
_cicd_engine: Optional[AutonomousCICDManagement] = None


def get_cicd_engine() -> AutonomousCICDManagement:
    """Get the CI/CD engine singleton"""
    global _cicd_engine
    if _cicd_engine is None:
        _cicd_engine = AutonomousCICDManagement()
    return _cicd_engine
